[{"uri":"/01introduction.html","title":" what is text summary","tags":[],"description":"","content":"text summary（文档摘要） 随着互联网产生的文本数据越来越多，文本信息过载问题日益严重，对各类文本进行一个“降 维”处理显得非常必要，文本摘要便是其中一个重要的手段。 文本摘要旨在将文本或文本集合转换为包含关键信息的简短摘要。 文本摘要按照输入类型可分为单文档摘要和多文档摘要。单文档摘要从给定的一个文档中生成摘要，多文档摘要从给定的一组主题相关的文档中生成摘要。 按照输出类型可分为抽取式摘要和生成式摘要。抽取式摘要从源文档中抽取关键句和关键词组成摘要，摘要全部来源于原文。生成式摘要根据原文，允许生成新的词语、短语来组成摘要。 按照有无监督数据可以分为有监督摘要和无监督摘要。 本文主要关注单文档、有监督、抽取式、生成式摘要。\n"},{"uri":"/01introduction/100algorithm.html","title":"1.1 算法概述","tags":[],"description":"","content":"使用场景 根据生成方式可以分为生成式摘要和抽取式摘要。\n 抽取式摘要：找到一个文档中最重要的几个句子并对其进行拼接。 生成式摘要：是一个序列生成问题，通过源文档序列, 生成序列摘要序列  本workshop主要覆盖以下几个算法，对比结果如下\n TextRANK MT5 CPT  "},{"uri":"/01introduction/200data.html","title":"1.2 数据集","tags":[],"description":"","content":"1.公开数据集 (英文)  XSUM 227k BBC articles CNN/Dailymail，93k articles from the CNN, 220k articles from the Daily Mail NEWSROOM，1.3M article-summary pairs written by authors and editors in the newsrooms of 38 major publications Multi-News，56k pairs of news articles and their human-written summaries from the http://sitenewser.com Gigaword，4M examples extracted from news articles，the task is to generate theheadline from the first sentence arXiv, PubMed，two long documentdatasets of scientific publications from http://arXiv.org (113k) andPubMed (215k). The task is to generate the abstract fromthe paper body. BIGPATENT，1.3 millionU.S. patents along with human summaries under nine patent classification categories WikiHow在线http://WikiHow.com网站上的大量说明数据集。 200k示例中的每一个示例都包含多个指令步骤段落以及一个摘要句子。 任务是从段落中生成串联的摘要句。 Reddit TIFU，120K posts of informal stories from the online discussion forum Reddit AESLC 18k email bodies and their subjects from the Enron corpus BillSum 23k USCongressional bills and human-written reference summaries from the 103rd-115th (1993-2018) sessions of Congress.  2.公开数据集 (中文)  哈工大的新浪微博短文本摘要 LCSTS 教育新闻自动摘要语料chinese_abstractive_corpus NLPCC 2017 task3 Single Document Summarization 娱乐新闻等 “神策杯”2018高校算法大师赛 清华 THUCNews 总数量：830749个样本； 标题：平均字数 19，字数标准差 4，最大字数 48，最小数字 4； 正文：平均字数 892，字数标准差 1012，最大字数 78796，最小数字 31；  "},{"uri":"/02textrank.html","title":"动手实验1: 基于Amazon SageMaker的TEXTRANK模型训练动手实验","tags":[],"description":"","content":"本章节中，将会部署一个带有gpu机型的Amazon SageMaker环境，然后使用TEXTRANK算法进行文档摘要。\nTEXTRANK介绍 将文本中的每个句子分别看做一个节点，如果两个句子有相似性，那么认为这两个句子对应的节点之间存在一条无向有权边。 公式中，Si,Sj分别表示两个句子词的个数总数，Wk表示句子中的词，那么分子部分的意思是同时出现在两个句子中的同一个词的个数，分母是对句子中词的个数求对数之和。分母这样设计可以遏制较长的句子在相似度计算上的优势。 我们可以根据以上相似度公式循环计算任意两个节点之间的相似度，根据阈值去掉两个节点之间相似度较低的边连接，构建出节点连接图，然后计算TextRank值，最后对所有TextRank值排序，选出TextRank值最高的几个节点对应的句子作为摘要。\nTextRank虽然考虑到了词之间的关系，但是仍然倾向于将频繁词作为关键词。\n"},{"uri":"/01introduction/300metrics.html","title":"1.3 评估指标","tags":[],"description":"","content":"文本生成目前的一大瓶颈是如何客观，准确的评价机器生成文本的质量。自动文档摘要评价方法大致分为两类：\n（1）内部评价方法：提供参考摘要，以参考摘要为基准评价系统摘要的质量。系统摘要与参考摘要越吻合，质量越高。\n（2）外部评价方法：不提供参考摘要，利用文档摘要代替原文档执行某个文档相关的应用。例如：文档检索、文档分类等，能够提高应用性能的摘要被认为是质量好的摘要。\n下面介绍两种比较简单的，经常用到的内部评价方法：\nEdmundson 适于抽取式文本摘要，比较机械文摘(自动文摘系统得到的文摘)与目标文摘(从原文中抽取的句子)的句子重合率的高低对系统摘要进行评价。\n计算公式：\n 重合率p = 匹配句子数/专家文摘句子数*100%  每一个机械文摘的重合率为按三个专家给出的文摘得到的重合率的平均值（其中，pi为相对于第i个专家的重合率，n为专家文摘总数）： ROUGE ROUGE（Recall-Oriented Understudy for Gisting Evaluation）基于摘要中n-gram的共现信息评价摘要，是一种面向n元词召回率的评价方法。 其中，Ref summaries表示标准摘要，count_match(n-gram)表示生成摘要和标准摘要中同时出现n-gram的个数，count(n-gram)表示参考摘要中出现的n-gram个数。\n"},{"uri":"/03bart.html","title":"动手实验2: 基于Amazon SageMaker的Huggingface训练一个BART英文摘要模型","tags":[],"description":"","content":"模型架构 BART架构由两个主要组件组成：编码器和解码器。\n 使用BERT的编码器组件，该组件用于从两个方向对输入语句进行编码，以获得更多上下文信息。 BART使用了来自GPT的解码器组件，该解码器组件用于重构噪声输入。然而，单词只能在leftward上下文使用，所以它不能学习双向互动。  除了解码器部分使用GeLU激活函数而非ReLU外，BART使用标准的序列到序列transformer架构，参数的初始化是从N(0.0, 0.02)开始的。BART有两种变体，基本模型在编码器和解码器中使用6层，而大型模型则每个使用12层。\n模型介绍 BART是一种采用序列到序列模型构建的降噪自编码器，适用于各种最终任务。它使用基于标准transformer的神经机器翻译架构。BART的预训练包括：\n1）使用噪声函数破坏文本; 2）学习序列到序列模型以重建原始文本。\n这些预训练步骤的主要优势在于：该模型可以灵活处理原始输入文本，并学会有效地重建文本。\n当为文本生成进行微调（fine-tuned）时，BART提供了健壮的性能，并且在理解任务中也能很好地工作。\n该模型的结果SOTA： 结果范例： BART在摘要任务上做得非常出色。以下示例摘要由BART生成。示例取自Wikinews文章。正如您所看到的，模型输出是流利且符合语法的英语。然而，模型输出也是高度抽象的，从输入中复制的短语很少 reference  paper： https://arxiv.org/pdf/1910.13461.pdf source code： https://github.com/pytorch/fairseq/tree/main/examples/bart  @article{lewis2019bart, title = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, author = {Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Veselin Stoyanov and Luke Zettlemoyer }, journal={arXiv preprint arXiv:1910.13461}, year = {2019}, }\n"},{"uri":"/04mt5.html","title":"动手实验3: 基于Amazon SageMaker的MT5中文摘要模型训练动手实验","tags":[],"description":"","content":"模型架构 Transformer For Text Summary：T5 (JMLR): 旨在开发NLP通用模型：将多个NLP下游任务抽象为text-to-text的任务，不同的任务使用不同的profix来代表，由于任务类型相同，故可以协同训练。\nmT5: A massively multilingualpre-trained text-to-text transformer 2021 google research\n大规模多语言T5预训练语言模型mT5，在覆盖101种语言的新的Common Crawl数据集上进行预训练，可直接适用于多语言场景，在各种基准测试集上展现出强大的性能。\nreference  paper： https://arxiv.org/pdf/2010.11934.pdf source code：https://github.com/google-research/multilingual-t5  @inproceedings{xue-etal-2021-mt5, title = \u0026ldquo;m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer\u0026rdquo;, author = \u0026ldquo;Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin\u0026rdquo;, booktitle = \u0026ldquo;Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\u0026rdquo;, month = jun, year = \u0026ldquo;2021\u0026rdquo;, address = \u0026ldquo;Online\u0026rdquo;, publisher = \u0026ldquo;Association for Computational Linguistics\u0026rdquo;, url = \u0026ldquo;https://aclanthology.org/2021.naacl-main.41\u0026quot;, doi = \u0026ldquo;10.18653/v1/2021.naacl-main.41\u0026rdquo;, pages = \u0026ldquo;483\u0026ndash;498\u0026rdquo; }\n"},{"uri":"/05cpt.html","title":"动手实验4: 基于Amazon SageMaker的CPT中文摘要模型训练动手实验型","tags":[],"description":"","content":"模型架构 CPT的具体结构可以看作一个输入，多个输出的非对称结构，主要包括三个部分:\n S-Enc (Shared Encoder): 共享Encoder，双向attention结构，建模输入文本。 U-Dec (Understanding Decoder): 理解用Decoder，双向attention结构，输入S-Enc得到的表示，输出MLM的结果。为模型增强理解任务。 G-Dec (Generation Decoder): 生成用Decoder，正如BART中的Decoder模块，利用encoder-decoder attention与S-Enc相连，用于生成。  模型介绍 该模型的结果SOTA：在生成任务上，在生成任务上CPT拥有与BART匹配的能力。在两个摘要数据集 (LCSTS和CSL) 和一个长文本生成数据集ADGEN测试的模型。 reference  paper： https://arxiv.org/pdf/2109.05729.pdf source code：https://github.com/fastnlp/CPT  @article{shao2021cpt, title={CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation}, author={Yunfan Shao and Zhichao Geng and Yitao Liu and Junqi Dai and Fei Yang and Li Zhe and Hujun Bao and Xipeng Qiu}, journal={arXiv preprint arXiv:2109.05729}, year={2021} }\n"},{"uri":"/02textrank/0201overview.html","title":"环境准备","tags":[],"description":"","content":"Amazon SageMaker 环境准备\n要完成本workshop操作步骤，您需要准备一台Amazon SageMaker笔记本实例：\n实例类型： p3.2xlarge 存储: 150G\n打开sagemaker，点击右上角的create notebook instance 填入如下配置信息，然后点击create notebook instance 在iam role中，选择使用系统为你创建的role 然后可以看到，提示笔记本正在创建中 等待一会儿，创建完成，点击create notebook instance\n"},{"uri":"/02textrank/0210component.html","title":" texkrank模型实验","tags":[],"description":"","content":"TextRank算法可以用来从文本中提取关键词和摘要（重要的句子）。TextRank4ZH是针对中文文本的TextRank算法的python算法实现。\n原理 TextRank的详细原理请参考：\n Mihalcea R, Tarau P. TextRank: Bringing order into texts[C]. Association for Computational Linguistics, 2004.\n 关键词提取 将原文本拆分为句子，在每个句子中过滤掉停用词（可选），并只保留指定词性的单词（可选）。由此可以得到句子的集合和单词的集合。\n每个单词作为pagerank中的一个节点。设定窗口大小为k，假设一个句子依次由下面的单词组成：\nw1, w2, w3, w4, w5, ..., wn w1, w2, ..., wk、w2, w3, ...,wk+1、w3, w4, ...,wk+2等都是一个窗口。在一个窗口中的任两个单词对应的节点之间存在一个无向无权的边。\n基于上面构成图，可以计算出每个单词节点的重要性。最重要的若干单词可以作为关键词。\n关键短语提取 参照关键词提取提取出若干关键词。若原文本中存在若干个关键词相邻的情况，那么这些关键词可以构成一个关键词组。\n例如，在一篇介绍支持向量机的文章中，可以找到关键词支持、向量、机，通过关键词组提取，可以得到支持向量机。\n摘要生成 将每个句子看成图中的一个节点，若两个句子之间有相似性，认为对应的两个节点之间有一个无向有权边，权值是相似度。\n通过pagerank算法计算得到的重要性最高的若干句子可以当作摘要。\n使用说明 类TextRank4Keyword、TextRank4Sentence在处理一段文本时会将文本拆分成4种格式：\n sentences：由句子组成的列表。 words_no_filter：对sentences中每个句子分词而得到的两级列表。 words_no_stop_words：去掉words_no_filter中的停止词而得到的二维列表。 words_all_filters：保留words_no_stop_words中指定词性的单词而得到的二维列表。  部署 git clone https://github.com/jackie930/TextRank4ZH.git cd TextRank4ZH/apps/text_summary_endpoint #注意这里您可以部署自己的ecr image，也可以我们打包好的公共镜像，本次实验会跳过build image步骤 ##sh build_and_push.sh Deploy endpoint on SageMaker endpoint_ecr_image=\u0026#34;847380964353.dkr.ecr.us-west-2.amazonaws.com/textrank\u0026#34; python create_endpoint.py \\ --endpoint_ecr_image_path ${endpoint_ecr_image} \\ --endpoint_name \u0026#39;textrank\u0026#39; \\ --instance_type \u0026#34;ml.t2.medium\u0026#34; 在部署结束后，看到SageMaker控制台生成了对应的endpoint,可以使用如下客户端代码测试调用\nfrom boto3.session import Session import json data={\u0026#34;data\u0026#34;: \u0026#34;台觀光局指出，先前已宣布2月1日至29日暫停旅行業組團前往中國大陸地區旅遊（含轉機前往其他地區旅遊）。為配合台疾病管制署提升港澳地區旅遊疫情等級為第二級，故2月6日起暫停台灣旅行社組團到港澳旅遊，但不含中轉港澳轉機到其他地區。台觀光局表示，持續配合台疾管署相關防疫作為，並視台疫情指揮中心發佈的疫情訊息綜合評估，隨時調整相關管制事宜。一、台籍人士：2月6日起，有陸港澳旅遊史者，需居家檢疫14天；申請獲准至港澳入境者，需自主健康管理14天。二、大陸人士：暫緩入境。三、港澳人士：2月7日起，入境後需居家檢疫14天。四、外籍人士：2月7日起，14天內曾經入境或居住於中國大陸、香港、澳門的外籍人士，暫緩入境。\u0026#34;} session = Session() runtime = session.client(\u0026#34;runtime.sagemaker\u0026#34;) response = runtime.invoke_endpoint( EndpointName=\u0026#39;textrank\u0026#39;, ContentType=\u0026#34;application/json\u0026#34;, Body=json.dumps(data), ) result = json.loads(response[\u0026#34;Body\u0026#34;].read()) print (result) 结果如下\n{'res': {'关键词列表': ['港澳', '需', '相關', '旅遊', '疫情', '地區', '管制', '光局', '大陸', '前往', '人士', '中國', '指出', '防疫', '中心', '綜合', '澳門', '表示', '持續', '配合'], '关键词权重': [0.06302083682777858, 0.03511030924850785, 0.033600141119327076, 0.030806166940925472, 0.029497862526199358, 0.02936235978105701, 0.027110814695381964, 0.02686551218821217, 0.02544770381895585, 0.025013920666751004, 0.02041656139425058, 0.019492944695266273, 0.01932912128465998, 0.01859340436433212, 0.018527042504337878, 0.018527042504337878, 0.017728167413452, 0.0175136542808295, 0.017279197108065837, 0.017116490039388914], '摘要列表': ['三、港澳人士：2月7日起，入境後需居家檢疫14天', '一、台籍人士：2月6日起，有陸港澳旅遊史者，需居家檢疫14天', '台觀光局指出，先前已宣布2月1日至29日暫停旅行業組團前往中國大陸地區旅遊（含轉機前往其他地區旅遊）'], '摘要位置index': [6, 3, 0], '摘要权重': [0.15060516235453508, 0.1490769827264645, 0.12750225410692212]}} CPU times: user 166 ms, sys: 15.5 ms, total: 181 ms Wall time: 4.53 s "},{"uri":"/03bart/0301train.html","title":" BART模型实验","tags":[],"description":"","content":"首先下载代码\ncd SageMaker git clone https://github.com/jackie930/notebooks.git 然后打开/home/ec2-user/SageMaker/notebooks/sagemaker/08_distributed_summarization_bart_t5/sagemaker-notebook.ipynb运行\n"},{"uri":"/04mt5/0301train.html","title":"MT5模型实验","tags":[],"description":"","content":"首先下载代码, ！注意，如果你已经执行过实验2bart，无需重复下载，直接打开即可。\ncd SageMaker git clone https://github.com/jackie930/notebooks.git 然后打开/home/ec2-user/SageMaker/notebooks/sagemaker/08_distributed_summarization_bart_t5/train-huggingface.ipynb运行。 注意在运行前，需要将待训练的数据‘meta_description.parquet' 上传到同样的文件夹内。\n训练开启后，可以看到产生了对应的训练日志\n及对应的训练任务\n大约25min完成训练，结束后可以直接部署模型文件为endpoint并进行推理。注意这里只用了1000条数据训练1轮，所以模型效果不佳。\n"},{"uri":"/05cpt/0501train.html","title":" CPT模型训练","tags":[],"description":"","content":"我们现在会用sagemaker进行一个cpt模型的本地训练（从头训练），使用ML.P3.2xlarge机型。\n数据准备 首先下载代码\ncd SageMaker git clone https://github.com/jackie930/CPT.git 然后打开文件 CPT/finetune/generation/train-meta-decription.ipynb，逐行运行。\n首先安装环境\n然后处理数据meta_description.parquet，切分train/test/dev， 注意这里的数据您需要手动倒入到相应路径。 注意这里，为了快速产生结果，我们只要用1000条数据训练，200条测试，200条验证。\n# use csv file to test x[:1000].to_csv(os.path.join(path,'train.csv'),index=False,encoding='utf-8') x[1000:1200].to_csv(os.path.join(path,'test.csv'),index=False,encoding='utf-8') x[1200:1400].to_csv(os.path.join(path,'dev.csv'),index=False,encoding='utf-8') 模型训练 接下来我们运行训练，为了演示目的，我们只运行一个epoch，大约需要10min\n!python run_gen_v2.py --model_path 'fnlp/cpt-large' --dataset hk01meta --data_dir demo_data --epoch '1' --batch_size '4' 训练完成后，会提示日志信息如下\n train  eval  predict   模型结果文件及相应的日志等信息会自动保存在output/hk01/1\n结果本地测试 我们可以直接用这个产生的模型文件进行本地推理。注意这里的模型文件地址的指定为你刚刚训练产生的。\n到这里，就完成了一个模型的训练过程。\n"},{"uri":"/05cpt/0502train.html","title":" CPT模型增强训练","tags":[],"description":"","content":"在完成本地训练后，接下来我们模拟一个增强训练过程。在我们刚才完成的训练任务中，产生了模型文件output/hk01meta/1目录下，然后，我们运行\n!python run_gen_v2.py --model_path 'output/hk01meta/1' --dataset hk01meta --data_dir demo_data --epoch '1' --batch_size '4' 我们可以看到，在训练时，日志中有如下的记录loading weights file output/hk01meta/2/pytorch_model.bin说明模型是导入了一个之前训练的基础版本，也可以通过训练的loss值判断出这个结果是增强训练产生的。\n同样的，我们可以利用本地推理进行测试。\n"},{"uri":"/05cpt/0503deploy.html","title":" CPT模型部署","tags":[],"description":"","content":"然后部署一个预置的endpoint\nendpoint_ecr_image=\u0026#34;847380964353.dkr.ecr.us-west-2.amazonaws.com/cpt\u0026#34; python create_endpoint.py \\ --endpoint_ecr_image_path ${endpoint_ecr_image} \\ --endpoint_name \u0026#39;cpt\u0026#39; \\ --instance_type \u0026#34;ml.p3.2xlarge\u0026#34; 在部署结束后，看到SageMaker控制台生成了对应的endpoint,可以使用如下客户端代码测试调用\n%%time from boto3.session import Session import json data={\u0026#34;data\u0026#34;: \u0026#34;台觀光局指出，先前已宣布2月1日至29日暫停旅行業組團前往中國大陸地區旅遊（含轉機前往其他地區旅遊）。為配合台疾病管制署提升港澳地區旅遊疫情等級為第二級，故2月6日起暫停台灣旅行社組團到港澳旅遊，但不含中轉港澳轉機到其他地區。台觀光局表示，持續配合台疾管署相關防疫作為，並視台疫情指揮中心發佈的疫情訊息綜合評估，隨時調整相關管制事宜。一、台籍人士：2月6日起，有陸港澳旅遊史者，需居家檢疫14天；申請獲准至港澳入境者，需自主健康管理14天。二、大陸人士：暫緩入境。三、港澳人士：2月7日起，入境後需居家檢疫14天。四、外籍人士：2月7日起，14天內曾經入境或居住於中國大陸、香港、澳門的外籍人士，暫緩入境。\u0026#34;} session = Session() runtime = session.client(\u0026#34;runtime.sagemaker\u0026#34;) response = runtime.invoke_endpoint( EndpointName=\u0026#39;cpt\u0026#39;, ContentType=\u0026#34;application/json\u0026#34;, Body=json.dumps(data), ) result = json.loads(response[\u0026#34;Body\u0026#34;].read()) print (result) 结果如下\n{'result': '[SEP] [CLS] 因 應 新 型 冠 狀 病 毒 肺 炎 （ 俗 稱 「 武 漢 肺 炎 」 ） 疫 情 ， 台 灣 觀 光 局 今 天 （ 7 日 ） 宣 布 ， 暫 停 台 灣 旅 行 社 組 團 到 港 澳 旅 遊 ， 但 不 含 中 轉 港 澳 轉 機 [SEP]', 'infer_time': '0:00:02.948025'} CPU times: user 169 ms, sys: 30.9 ms, total: 200 ms Wall time: 3.42 s "},{"uri":"/","title":"AWS Datalab- 文档摘要","tags":[],"description":"","content":"Author\n JUNYI LIU (AWS GCR Applied Scientist)  概述 本次workshop分为几个部分\n 背景介绍- what is text summary？ 基于Amazon SageMaker的TEXTRANK模型训练动手实验 基于Amazon SageMaker的Huggingface训练一个BART英文摘要模型  模型训练 模型部署   基于Amazon SageMaker的MT5模型训练动手实验  本地模型训练 启动模型训练任务 模型部署   基于Amazon SageMaker的CPT模型训练动手实验  模型训练 模型增强训练 模型部署    本次 workshop 前提 本次 workshop 建议在 US-WEST-2 Region 使用。为了演示方便，所以本 workshop 所有的演示都会以US-WEST-2 Region 为例。\n"},{"uri":"/categories.html","title":"Categories","tags":[],"description":"","content":""},{"uri":"/tags.html","title":"Tags","tags":[],"description":"","content":""}]